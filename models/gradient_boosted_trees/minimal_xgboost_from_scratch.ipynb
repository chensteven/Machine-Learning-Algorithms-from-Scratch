{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation follows the basic steps of the XGBoost algorithm, including:\n",
    "\n",
    "Initializing the model predictions to the mean of the training labels\n",
    "Looping over a fixed number of trees, and for each tree:\n",
    "a. Computing the gradient and hessian of the loss function for the current predictions\n",
    "b. Fitting a decision tree to the negative gradient of the loss function\n",
    "c. Updating the model predictions with the current tree's weighted predictions\n",
    "Computing the final model predictions and evaluating the performance.\n",
    "Note that this is a simplified implementation and does not include all the advanced features and optimizations of the XGBoost algorithm found in more complete implementations such as the XGBoost library for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=123)\n",
    "\n",
    "# Define the XGBoost model parameters\n",
    "n_trees = 10\n",
    "max_depth = 5\n",
    "learning_rate = 0.1\n",
    "reg_lambda = 1\n",
    "\n",
    "# Initialize the model predictions to the mean of the training labels\n",
    "mean_y = np.mean(y_train)\n",
    "pred_train = np.full((len(y_train),), mean_y)\n",
    "pred_test = np.full((len(y_test),), mean_y)\n",
    "\n",
    "# Loop over the number of trees\n",
    "for i in range(n_trees):\n",
    "    # Compute the gradient and hessian of the loss function for the current predictions\n",
    "    grad = y_train - pred_train\n",
    "    hess = np.ones_like(y_train)\n",
    "\n",
    "    # Fit a decision tree to the negative gradient of the loss function\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    tree.fit(X_train, -grad)\n",
    "\n",
    "    # Compute the predictions for the training and testing sets\n",
    "    pred_train += learning_rate * tree.predict(X_train)\n",
    "    pred_test += learning_rate * tree.predict(X_test)\n",
    "\n",
    "    # Apply L2 regularization to the tree weights\n",
    "    tree_weight = reg_lambda / (1.0 + reg_lambda * i)\n",
    "\n",
    "    # Update the model predictions with the current tree's weighted predictions\n",
    "    pred_train = (1 - tree_weight) * pred_train + tree_weight * pred_test\n",
    "    pred_test = (1 - tree_weight) * pred_test + tree_weight * tree.predict(X_test)\n",
    "\n",
    "# Compute the final model predictions and evaluate the performance\n",
    "y_pred = pred_test\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"XGBoost MSE:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
